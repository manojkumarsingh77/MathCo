{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use an Azure Machine Learning Studio Experiment to test how batch size affects model accuracy in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to authenticate the notebook to our Workspace. If you are running the notebook in Azure, the config is populated for you. If you are running the notebook remotely, you must download the workspace configuration (config.json) and place it in the current directory.\n",
    "\n",
    "You may get a prompt to authenticate to Azure. Click the link and use the provided code to authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is available in the [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/catalog/mnist/). We'll import it and create a funtion to split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import MNIST\n",
    "\n",
    "def get_MNIST_values(train=True):\n",
    "    dataFilter = 'train' if train else 'test'\n",
    "    df = MNIST.get_tabular_dataset(enable_telemetry=False, dataset_filter=dataFilter).to_pandas_dataframe()\n",
    "    x = df.drop('label', axis=1).values.astype('float32') / 255\n",
    "    y = df.filter(items=[\"label\"]).values.astype('int64')\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = get_MNIST_values(train=True)\n",
    "x_test, y_test = get_MNIST_values(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our examples and labels are both numpy N-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\")\n",
    "print(x_train.shape)\n",
    "print(x_train.dtype)\n",
    "\n",
    "print(\"\\nTest:\")\n",
    "print(x_test.shape)\n",
    "print(x_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 60,000 examples in the training set and 10,000 examples in the test set, consisting of 32-bit floating point numbers.\n",
    "\n",
    "MNIST is a monochromatic image data set, which means the raw data has int values between 0 and 255 for pixel brightness. The `get_MNIST_values` method above converts the integers to a float value between 0 and 1, which works better for training.\n",
    "\n",
    "Normally, the image would be an array of 28x28 pixels, but the `MNIST.get_tabular_dataset` method has flattened that to a single row containing all 784 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are integers, which are the correct values for the number in the hand-written digit.\n",
    "\n",
    "**Let's view a sample of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "count = 0\n",
    "sample_size = 50\n",
    "cols = 10\n",
    "rows = math.ceil(sample_size / cols)\n",
    "plt.figure(figsize=(16, rows))\n",
    "plt.subplots_adjust(top=1.5)\n",
    "for i in np.random.permutation(x_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(rows, cols, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=12, y=-2, s=y_train[i][0], fontsize=16)\n",
    "    plt.imshow(x_train[i].reshape(28,28), cmap=plt.cm.Greys)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert the N-Dimensional Arrays to a TensorFlow Dataset. We create a method to generate the dataset since the batch size of the dataset is what we are testing. The test data does not need shuffled, so we accept a parameter for the data's purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_values(data, labels, batch_size=32, train=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels)).batch(batch_size)\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_dataset_from_values(x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last bit of data setup is to create a model-generating function. We need a new model for each experiment, rather than continuing to train the same model with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a place to keep track of our experiments. Since we're working with MNIST, we'll call it \"mnist\".\n",
    "\n",
    "**Note:** As of the time of writing, Experiments can't be deleted. They can be archived, which hides them from the UI, but the results are currently permanent within the Machine Learning Workspace. If you are developing new code, you should either use a separate development Machine Learning Workspace (a reasonable practice either way) or use an experiment name that you don't care to lose, such as appending a timestamp or random characters to the end (\"mnist-ofj23\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment = Experiment(workspace=ws, name=\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to start the experiment!\n",
    "\n",
    "We'll try doubling our batch_sizes in multiples of 16 to see if the larger values have any affect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "\n",
    "for size in batch_size:\n",
    "    run = experiment.start_logging()\n",
    "    run.log(\"batch_size\", size)\n",
    "    \n",
    "    #Create a new model and batch our training data with the current experiment size.\n",
    "    model = get_model()\n",
    "    traindata = get_dataset_from_values(x_train, y_train, batch_size = size)\n",
    "    \n",
    "    #Train the model\n",
    "    model.fit(traindata, epochs=3)\n",
    "    \n",
    "    #Evaluate the model\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    \n",
    "    #Add the loss and accuracy information to the experiment for viewing later\n",
    "    run.log(\"loss\", float('%.3f' % loss))\n",
    "    run.log(\"accuracy\", float('%.3f' % (acc * 100)))\n",
    "        \n",
    "    run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of your experiments in Azure Machine Learning Studio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
